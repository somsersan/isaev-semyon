"""
–£–ø—Ä–æ—â—ë–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è Contextual Bandit –∑–∞–¥–∞—á–∏.

–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
1. –ú–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ config.yaml
3. –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∫–∞–∫ baseline
4. –ë–µ–∑ –ø–µ—Ä–µ—É—Å–ª–æ–∂–Ω–µ–Ω–∏—è
"""
import logging
import os
import random
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import yaml
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
ID_COL = "id"
ACTION_COL = "segment"
REWARD_COL = "visit"
ACTIONS = ("Mens E-Mail", "Womens E-Mail", "No E-Mail")
ACTION_TO_INDEX = {action: idx for idx, action in enumerate(ACTIONS)}

# –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
NUMERIC_FEATURES = ["recency", "history"]
BINARY_FEATURES = ["mens", "womens", "newbie"]
CATEGORICAL_FEATURES = ["zip_code", "channel", "history_segment"]


def load_config(config_path: str = "config.yaml") -> dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML —Ñ–∞–π–ª–∞."""
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    return config


def set_seed(seed: int) -> None:
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏."""
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def setup_logging(level: str = "INFO") -> None:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    logging.basicConfig(
        format="[%(levelname)s] %(message)s",
        level=getattr(logging, level.upper()),
    )


def load_data(config: dict) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ train –∏ test –¥–∞–Ω–Ω—ã—Ö."""
    train_df = pd.read_csv(config["data"]["train_path"])
    test_df = pd.read_csv(config["data"]["test_path"])
    logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: train={len(train_df)}, test={len(test_df)}")
    return train_df, test_df


def build_preprocessor() -> ColumnTransformer:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ –±–µ–∑ feature engineering.
    """
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), NUMERIC_FEATURES),
            ("bin", "passthrough", BINARY_FEATURES),
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), CATEGORICAL_FEATURES),
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )
    return preprocessor


def create_model(config: dict):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥–∞."""
    model_type = config["model"]["type"]
    seed = config["seed"]
    
    if model_type == "logistic":
        params = config["model"]["logistic"]
        return LogisticRegression(
            max_iter=params["max_iter"],
            C=params["C"],
            solver=params["solver"],
            random_state=seed,
        )
    elif model_type == "random_forest":
        params = config["model"]["random_forest"]
        return RandomForestClassifier(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"],
            min_samples_leaf=params["min_samples_leaf"],
            min_samples_split=params["min_samples_split"],
            random_state=seed,
            n_jobs=-1,
        )
    elif model_type == "extra_trees":
        params = config["model"]["extra_trees"]
        return ExtraTreesClassifier(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"],
            min_samples_leaf=params["min_samples_leaf"],
            min_samples_split=params["min_samples_split"],
            bootstrap=False,
            random_state=seed,
            n_jobs=-1,
        )
    else:
        raise ValueError(f"Unknown model type: {model_type}")


def fit_reward_models(
    train_df: pd.DataFrame,
    preprocessor: ColumnTransformer,
    config: dict,
) -> Dict[str, Pipeline]:
    """
    –û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.
    Direct Method: P(reward=1 | x, action)
    """
    models = {}
    
    for action in ACTIONS:
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
        mask = train_df[ACTION_COL] == action
        X = train_df.loc[mask]
        y = train_df.loc[mask, REWARD_COL]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
        model = create_model(config)
        pipeline = Pipeline([
            ("preprocess", preprocessor),
            ("model", model),
        ])
        
        # –û–±—É—á–µ–Ω–∏–µ
        pipeline.fit(X, y)
        models[action] = pipeline
        
        logging.info(f"–û–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è '{action}': {mask.sum()} –ø—Ä–∏–º–µ—Ä–æ–≤, "
                    f"reward rate = {y.mean():.3f}")
    
    return models


def predict_q_values(df: pd.DataFrame, models: Dict[str, Pipeline]) -> np.ndarray:
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π reward=1) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.
    
    Returns:
        Array shape [n_samples, n_actions] —Å Q(x, a)
    """
    q_columns = []
    for action in ACTIONS:
        model = models[action]
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å reward=1
        probs = model.predict_proba(df)[:, 1]
        q_columns.append(probs)
    
    return np.column_stack(q_columns)


def make_policy_greedy(
    q_values: np.ndarray,
    epsilon: float = 0.05,
) -> np.ndarray:
    """
    –ñ–∞–¥–Ω–∞—è (–ø–æ—á—Ç–∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∞—è) –ø–æ–ª–∏—Ç–∏–∫–∞ —Å epsilon-greedy.
    
    œÄ(a|x) = 1 - Œµ,  –µ—Å–ª–∏ a = argmax Q(x,a')
             Œµ/n,    –∏–Ω–∞—á–µ
    
    Args:
        q_values: Q-–∑–Ω–∞—á–µ–Ω–∏—è shape [n_samples, n_actions]
        epsilon: –î–æ–ª—è –¥–ª—è non-greedy –¥–µ–π—Å—Ç–≤–∏–π
    
    Returns:
        Policy probabilities shape [n_samples, n_actions]
    """
    n_samples, n_actions = q_values.shape
    
    # –ñ–∞–¥–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
    greedy_actions = np.argmax(q_values, axis=1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å uniform epsilon
    policy = np.full((n_samples, n_actions), epsilon / n_actions)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∞ –∂–∞–¥–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
    policy[np.arange(n_samples), greedy_actions] += (1.0 - epsilon)
    
    # –†–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    policy = policy / policy.sum(axis=1, keepdims=True)
    
    return policy


def make_policy_softmax(
    q_values: np.ndarray,
    temperature: float = 1.0,
    min_prob: float = 0.01,
) -> np.ndarray:
    """
    Softmax –ø–æ–ª–∏—Ç–∏–∫–∞ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π.
    
    œÄ(a|x) = softmax(Q/T)
    
    Args:
        q_values: Q-–∑–Ω–∞—á–µ–Ω–∏—è shape [n_samples, n_actions]
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è softmax (–Ω–∏–∂–µ = –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω–æ)
        min_prob: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    
    Returns:
        Policy probabilities shape [n_samples, n_actions]
    """
    # Softmax —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π
    logits = q_values / max(temperature, 1e-6)
    logits = logits - logits.max(axis=1, keepdims=True)  # numerical stability
    exp_logits = np.exp(logits)
    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)
    
    # –ö–ª–∏–ø–ø–∏–Ω–≥ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    n_actions = probs.shape[1]
    max_prob = 1.0 - (n_actions - 1) * min_prob
    probs = np.clip(probs, min_prob, max_prob)
    
    # –†–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    probs = probs / probs.sum(axis=1, keepdims=True)
    
    return probs


def make_policy(q_values: np.ndarray, config: dict) -> np.ndarray:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    
    Args:
        q_values: Q-–∑–Ω–∞—á–µ–Ω–∏—è
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    
    Returns:
        Policy probabilities
    """
    policy_type = config["policy"]["type"]
    
    if policy_type == "greedy":
        return make_policy_greedy(q_values, epsilon=config["policy"]["epsilon"])
    elif policy_type == "softmax":
        return make_policy_softmax(
            q_values,
            temperature=config["policy"]["temperature"],
            min_prob=config["policy"]["min_prob"],
        )
    else:
        raise ValueError(f"Unknown policy type: {policy_type}")


def snips_score(
    policy_probs: np.ndarray,
    actions: pd.Series,
    rewards: pd.Series,
    mu: float = 1/3,
) -> float:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ SNIPS (Self-Normalized Importance Sampling) –º–µ—Ç—Ä–∏–∫–∏.
    
    SNIPS = Œ£(œÄ(a|x)/Œº * r) / Œ£(œÄ(a|x)/Œº)
    """
    actions_arr = actions.values
    rewards_arr = rewards.values
    
    # –ò–Ω–¥–µ–∫—Å—ã –¥–µ–π—Å—Ç–≤–∏–π
    action_indices = np.array([ACTION_TO_INDEX[a] for a in actions_arr])
    
    # œÄ(a_logged | x)
    pi_logged = policy_probs[np.arange(len(policy_probs)), action_indices]
    
    # –í–∞–∂–Ω–æ—Å—Ç—å (importance weights)
    weights = pi_logged / mu
    
    # SNIPS
    numerator = np.sum(weights * rewards_arr)
    denominator = np.sum(weights)
    
    return numerator / denominator if denominator > 0 else 0.0


def best_static_policy_value(actions: pd.Series, rewards: pd.Series, mu: float = 1/3) -> float:
    """
    –ó–Ω–∞—á–µ–Ω–∏–µ –ª—É—á—à–µ–π —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (–≤—ã–±–∏—Ä–∞–µ—Ç –æ–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ –≤—Å–µ–≥–¥–∞).
    –≠—Ç–æ baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
    
    Best Static IPS = max_a [ E[r | a] ] = max_a [ mean(rewards where action=a) ]
    """
    best_value = -np.inf
    
    for action in ACTIONS:
        mask = actions == action
        if mask.sum() == 0:
            continue
        
        # –°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
        # IPS –æ—Ü–µ–Ω–∫–∞: (sum(r)/mu) / (count/mu) = sum(r) / count = mean(r)
        value = rewards[mask].mean()
        best_value = max(best_value, value)
    
    return best_value


def create_submission(policy_probs: np.ndarray, ids: pd.Series) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞."""
    return pd.DataFrame({
        ID_COL: ids,
        "p_mens_email": policy_probs[:, ACTION_TO_INDEX["Mens E-Mail"]],
        "p_womens_email": policy_probs[:, ACTION_TO_INDEX["Womens E-Mail"]],
        "p_no_email": policy_probs[:, ACTION_TO_INDEX["No E-Mail"]],
    })


def save_submission(submission_df: pd.DataFrame, path: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ submission —Ñ–∞–π–ª–∞."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    submission_df.to_csv(path, index=False)
    logging.info(f"‚úÖ Submission —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
    config = load_config("config.yaml")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    set_seed(config["seed"])
    setup_logging(config["logging"]["level"])
    
    logging.info("=" * 60)
    logging.info("CONTEXTUAL BANDIT - SIMPLIFIED BASELINE")
    logging.info("=" * 60)
    logging.info(f"–ú–æ–¥–µ–ª—å: {config['model']['type']}")
    logging.info(f"–ü–æ–ª–∏—Ç–∏–∫–∞: {config['policy']['type']}")
    if config['policy']['type'] == 'greedy':
        logging.info(f"Epsilon: {config['policy']['epsilon']}")
    elif config['policy']['type'] == 'softmax':
        logging.info(f"Temperature: {config['policy']['temperature']}")
    logging.info(f"Seed: {config['seed']}")
    logging.info("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_df, test_df = load_data(config)
    
    # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    preprocessor = build_preprocessor()
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (Direct Method)
    logging.info("\nüîß –û–±—É—á–µ–Ω–∏–µ reward –º–æ–¥–µ–ª–µ–π...")
    models = fit_reward_models(train_df, preprocessor, config)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ train –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    logging.info("\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö...")
    train_q_values = predict_q_values(train_df, models)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏
    train_policy = make_policy(train_q_values, config)
    
    # –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏
    snips_value = snips_score(train_policy, train_df[ACTION_COL], train_df[REWARD_COL], config["mu"])
    best_static = best_static_policy_value(train_df[ACTION_COL], train_df[REWARD_COL], config["mu"])
    score = snips_value - best_static
    
    logging.info(f"\nüìà –ú–ï–¢–†–ò–ö–ò:")
    logging.info(f"  SNIPS: {snips_value:.5f}")
    logging.info(f"  Best Static: {best_static:.5f}")
    logging.info(f"  Score (SNIPS - Best Static): {score:.5f}")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ test
    logging.info("\nüéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission...")
    test_q_values = predict_q_values(test_df, models)
    test_policy = make_policy(test_q_values, config)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ submission
    submission_df = create_submission(test_policy, test_df[ID_COL])
    save_submission(submission_df, config["data"]["submission_path"])
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏
    logging.info(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ (test):")
    for i, action in enumerate(ACTIONS):
        mean_prob = test_policy[:, i].mean()
        logging.info(f"  {action}: {mean_prob:.3f} (—Å—Ä–µ–¥–Ω–µ–µ)")
    
    logging.info("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()

