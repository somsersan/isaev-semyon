"""
–û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —Ä–µ—à–µ–Ω–∏–µ–º —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è
–ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–µ—Å—å –≤–∞—à –∫–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
"""
import logging
import os
import random
from typing import Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
ID_COL = "id"
ACTION_COL = "segment"
REWARD_COL = "visit"
ACTIONS = ("Mens E-Mail", "Womens E-Mail", "No E-Mail")
ACTION_TO_INDEX = {action: idx for idx, action in enumerate(ACTIONS)}

# –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
NUMERIC_FEATURES = ["recency", "history"]
BINARY_FEATURES = ["mens", "womens", "newbie"]
CATEGORICAL_FEATURES = ["zip_code", "channel", "history_segment"]


class CTRTargetEncoder(BaseEstimator, TransformerMixin):
    """
    CTR (target) —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –ù–∞ –≤—ã—Ö–æ–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —á–∏—Å–ª–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞
    —Å –∞–ø—Ä–∏–æ—Ä–Ω–æ-—Å–≥–ª–∞–∂–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π P(reward=1 | category).
    """
    def __init__(self, columns, alpha: float = 5.0, handle_unknown: str = "global_mean"):
        # –í–ê–ñ–ù–û: –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ __init__, —á—Ç–æ–±—ã –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å sklearn.clone
        self.columns = columns
        self.alpha = float(alpha)
        self.handle_unknown = handle_unknown
        self.global_mean_ = None
        self.mapping_ = {}
        self.feature_names_out_ = None
    
    def fit(self, X, y=None):
        if y is None:
            raise ValueError("CTRTargetEncoder —Ç—Ä–µ–±—É–µ—Ç y (—Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é) –ø—Ä–∏ fit.")
        X_df = pd.DataFrame(X).copy() if not isinstance(X, pd.DataFrame) else X
        y_arr = pd.Series(y).astype(float)
        columns = list(self.columns)
        
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π CTR
        self.global_mean_ = float(y_arr.mean()) if len(y_arr) > 0 else 0.0
        self.mapping_ = {}
        feature_names_out = []
        
        for col in columns:
            # –ì—Ä—É–ø–ø–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = (
                X_df[[col]]
                .assign(target=y_arr.values)
                .groupby(col)["target"]
                .agg(["sum", "count"])
            )
            # –°–≥–ª–∞–∂–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            stats["ctr"] = (stats["sum"] + self.alpha * self.global_mean_) / (stats["count"] + self.alpha)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
            self.mapping_[col] = stats["ctr"].to_dict()
            feature_names_out.append(f"{col}_ctr")
        
        self.feature_names_out_ = np.array(feature_names_out, dtype=object)
        return self
    
    def transform(self, X):
        X_df = pd.DataFrame(X).copy() if not isinstance(X, pd.DataFrame) else X
        encoded_cols = []
        for col in list(self.columns):
            mapping = self.mapping_.get(col, {})
            col_encoded = X_df[col].map(mapping)
            if self.handle_unknown == "global_mean":
                col_encoded = col_encoded.fillna(self.global_mean_)
            else:
                # fallback –Ω–∞ 0.0 –µ—Å–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∞ –∏–Ω–∞—è; –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º global_mean
                col_encoded = col_encoded.fillna(0.0)
            encoded_cols.append(col_encoded.astype(float).values.reshape(-1, 1))
        if not encoded_cols:
            return np.empty((len(X_df), 0))
        return np.hstack(encoded_cols)
    
    def get_feature_names_out(self, input_features=None):
        cols = list(self.columns)
        return self.feature_names_out_ if self.feature_names_out_ is not None else np.array([f"{c}_ctr" for c in cols])


def set_seed(seed: int) -> None:
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏."""
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)


def setup_logging(level: str = "INFO") -> None:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    logging.basicConfig(
        format="[%(levelname)s] %(message)s",
        level=getattr(logging, level.upper()),
    )


def load_data(config: dict) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ train –∏ test –¥–∞–Ω–Ω—ã—Ö."""
    train_df = pd.read_csv(config["data"]["train_path"])
    test_df = pd.read_csv(config["data"]["test_path"])
    logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: train={len(train_df)}, test={len(test_df)}")
    return train_df, test_df


def build_preprocessor() -> ColumnTransformer:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –û–±–Ω–æ–≤–ª–µ–Ω–æ: –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CTR target encoding.
    """
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), NUMERIC_FEATURES),
            ("bin", "passthrough", BINARY_FEATURES),
            ("cat_ctr", CTRTargetEncoder(CATEGORICAL_FEATURES, alpha=5.0, handle_unknown="global_mean"), CATEGORICAL_FEATURES),
        ],
        remainder="drop",
        sparse_threshold=0.0,
    )
    return preprocessor


def create_model(config: dict):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥–∞."""
    model_type = config["model"]["type"]
    seed = config["seed"]
    
    if model_type == "logistic":
        params = config["model"]["logistic"]
        return LogisticRegression(
            max_iter=params["max_iter"],
            C=params["C"],
            solver=params["solver"],
            random_state=seed,
        )
    elif model_type == "random_forest":
        params = config["model"]["random_forest"]
        return RandomForestClassifier(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"],
            min_samples_leaf=params["min_samples_leaf"],
            min_samples_split=params["min_samples_split"],
            random_state=seed,
            n_jobs=-1,
        )
    elif model_type == "extra_trees":
        params = config["model"]["extra_trees"]
        return ExtraTreesClassifier(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"],
            min_samples_leaf=params["min_samples_leaf"],
            min_samples_split=params["min_samples_split"],
            bootstrap=False,
            random_state=seed,
            n_jobs=-1,
        )
    else:
        raise ValueError(f"Unknown model type: {model_type}")


def fit_reward_models(
    train_df: pd.DataFrame,
    preprocessor: ColumnTransformer,
    config: dict,
) -> Dict[str, Pipeline]:
    """
    –û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.
    Direct Method: P(reward=1 | x, action)
    """
    models = {}
    
    for action in ACTIONS:
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
        mask = train_df[ACTION_COL] == action
        X = train_df.loc[mask]
        y = train_df.loc[mask, REWARD_COL]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
        model = create_model(config)
        pipeline = Pipeline([
            ("preprocess", preprocessor),
            ("model", model),
        ])
        
        # –û–±—É—á–µ–Ω–∏–µ
        pipeline.fit(X, y)
        models[action] = pipeline
        
        logging.info(f"–û–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è '{action}': {mask.sum()} –ø—Ä–∏–º–µ—Ä–æ–≤, "
                    f"reward rate = {y.mean():.3f}")
    
    return models


def predict_q_values(df: pd.DataFrame, models: Dict[str, Pipeline]) -> np.ndarray:
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π reward=1) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.
    
    Returns:
        Array shape [n_samples, n_actions] —Å Q(x, a)
    """
    q_columns = []
    for action in ACTIONS:
        model = models[action]
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å reward=1
        probs = model.predict_proba(df)[:, 1]
        q_columns.append(probs)
    
    return np.column_stack(q_columns)


def make_policy_greedy(
    q_values: np.ndarray,
    epsilon: float = 0.05,
) -> np.ndarray:
    """
    –ñ–∞–¥–Ω–∞—è (–ø–æ—á—Ç–∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∞—è) –ø–æ–ª–∏—Ç–∏–∫–∞ —Å epsilon-greedy.
    
    œÄ(a|x) = 1 - Œµ,  –µ—Å–ª–∏ a = argmax Q(x,a')
             Œµ/n,    –∏–Ω–∞—á–µ
    
    Args:
        q_values: Q-–∑–Ω–∞—á–µ–Ω–∏—è shape [n_samples, n_actions]
        epsilon: –î–æ–ª—è –¥–ª—è non-greedy –¥–µ–π—Å—Ç–≤–∏–π
    
    Returns:
        Policy probabilities shape [n_samples, n_actions]
    """
    n_samples, n_actions = q_values.shape
    
    # –ñ–∞–¥–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
    greedy_actions = np.argmax(q_values, axis=1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å uniform epsilon
    policy = np.full((n_samples, n_actions), epsilon / n_actions)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∞ –∂–∞–¥–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
    policy[np.arange(n_samples), greedy_actions] += (1.0 - epsilon)
    
    # –†–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    policy = policy / policy.sum(axis=1, keepdims=True)
    
    return policy


def make_policy_softmax(
    q_values: np.ndarray,
    temperature: float = 1.0,
    min_prob: float = 0.01,
) -> np.ndarray:
    """
    Softmax –ø–æ–ª–∏—Ç–∏–∫–∞ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π.
    
    œÄ(a|x) = softmax(Q/T)
    
    Args:
        q_values: Q-–∑–Ω–∞—á–µ–Ω–∏—è shape [n_samples, n_actions]
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è softmax (–Ω–∏–∂–µ = –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω–æ)
        min_prob: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    
    Returns:
        Policy probabilities shape [n_samples, n_actions]
    """
    # Softmax —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π
    logits = q_values / max(temperature, 1e-6)
    logits = logits - logits.max(axis=1, keepdims=True)  # numerical stability
    exp_logits = np.exp(logits)
    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)
    
    # –ö–ª–∏–ø–ø–∏–Ω–≥ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    n_actions = probs.shape[1]
    max_prob = 1.0 - (n_actions - 1) * min_prob
    probs = np.clip(probs, min_prob, max_prob)
    
    # –†–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    probs = probs / probs.sum(axis=1, keepdims=True)
    
    return probs


def make_policy(q_values: np.ndarray, config: dict) -> np.ndarray:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    
    Args:
        q_values: Q-–∑–Ω–∞—á–µ–Ω–∏—è
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    
    Returns:
        Policy probabilities
    """
    policy_type = config["policy"]["type"]
    
    if policy_type == "greedy":
        return make_policy_greedy(q_values, epsilon=config["policy"]["epsilon"])
    elif policy_type == "softmax":
        return make_policy_softmax(
            q_values,
            temperature=config["policy"]["temperature"],
            min_prob=config["policy"]["min_prob"],
        )
    else:
        raise ValueError(f"Unknown policy type: {policy_type}")


def compute_action_stats(train_df: pd.DataFrame) -> Tuple[Dict[str, float], Dict[str, int], str]:
    """
    –ü–æ–¥—Å—á—ë—Ç —Å—Ä–µ–¥–Ω–∏—Ö –Ω–∞–≥—Ä–∞–¥ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ –∫–∞–∂–¥–æ–º—É –¥–µ–π—Å—Ç–≤–∏—é (segment).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - —Å—Ä–µ–¥–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º
      - –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º
      - –ª—É—á—à–∞—è —Ä—É–∫–∞ (–¥–µ–π—Å—Ç–≤–∏–µ) –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É reward
    """
    reward_means: Dict[str, float] = {}
    counts: Dict[str, int] = {}
    best_action: str = ACTIONS[0]
    best_mean = -np.inf
    for action in ACTIONS:
        mask = train_df[ACTION_COL] == action
        cnt = int(mask.sum())
        counts[action] = cnt
        if cnt > 0:
            mean_reward = float(train_df.loc[mask, REWARD_COL].mean())
        else:
            mean_reward = -np.inf
        reward_means[action] = mean_reward
        if mean_reward > best_mean:
            best_mean = mean_reward
            best_action = action
    return reward_means, counts, best_action


def make_rl_wrapped_policy(
    q_values: np.ndarray,
    baseline_best_action_idx: int,
    trained_action_mask: np.ndarray,
    epsilon: float = 0.1,
    override_delta: float = 0.15,
) -> np.ndarray:
    """
    –õ—ë–≥–∫–∞—è RL-–æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ –ø–æ–ª–∏—Ç–∏–∫–æ–π:
      1) –ù–∞—á–∏–Ω–∞—Ç—å —Å one-hot –Ω–∞ –ª—É—á—à—É—é —Ä—É–∫—É (–ø–æ train —Å—Ä–µ–¥–Ω–µ–º—É reward)
      2) –†–∞–∑—Ä–µ—à–∞—Ç—å ML –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
         (max_q - q_best) > override_delta –∏ –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Ä—É–∫–∏ –æ–±—É—á–µ–Ω–∞
      3) –í–Ω–µ—Å—Ç–∏ Œµ-—ç–∫—Å–ø–ª–æ—Ä–∞—Ü–∏—é: —Å–º–µ—à–∏–≤–∞–µ–º —Å uniform —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é Œµ
    """
    n_samples, n_actions = q_values.shape
    assert n_actions == len(ACTIONS)
    uniform = np.full((n_samples, n_actions), 1.0 / n_actions, dtype=float)
    # –ë–∞–∑–æ–≤—ã–π one-hot –Ω–∞ –ª—É—á—à—É—é —Ä—É–∫—É
    base = np.zeros((n_samples, n_actions), dtype=float)
    base[:, baseline_best_action_idx] = 1.0
    # –ö–∞–Ω–¥–∏–¥–∞—Ç –æ—Ç ML
    argmax_actions = np.argmax(q_values, axis=1)
    best_q = q_values[:, baseline_best_action_idx]
    max_q = q_values[np.arange(n_samples), argmax_actions]
    allow_override = (max_q - best_q) > override_delta
    # –£—á–∏—Ç—ã–≤–∞–µ–º, —á—Ç–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–±—É—á–µ–Ω–Ω—É—é —Ä—É–∫—É
    trained_idx = np.array(trained_action_mask, dtype=bool)
    can_override_to_trained = trained_idx[argmax_actions]
    do_override = allow_override & can_override_to_trained
    # –ü–æ—Å—Ç—Ä–æ–∏–º –∏—Ç–æ–≥–æ–≤—ã–π one-hot –¥–æ —Å–º–µ—à–∏–≤–∞–Ω–∏—è —Å uniform
    final_one_hot = base.copy()
    rows_to_override = np.where(do_override)[0]
    if rows_to_override.size > 0:
        final_one_hot[rows_to_override, baseline_best_action_idx] = 0.0
        final_one_hot[rows_to_override, argmax_actions[rows_to_override]] = 1.0
    # Œµ-—ç–∫—Å–ø–ª–æ—Ä–∞—Ü–∏—è: —Å–º–µ—à–∏–≤–∞–Ω–∏–µ —Å uniform
    policy = (1.0 - epsilon) * final_one_hot + epsilon * uniform
    # –ß–∏—Å–ª–µ–Ω–Ω–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ–º –∏ —Ä–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    policy = np.clip(policy, 1e-9, 1.0)
    policy = policy / policy.sum(axis=1, keepdims=True)
    return policy


def snips_score(
    policy_probs: np.ndarray,
    actions: pd.Series,
    rewards: pd.Series,
    mu: float = 1/3,
) -> float:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ SNIPS (Self-Normalized Importance Sampling) –º–µ—Ç—Ä–∏–∫–∏.
    
    SNIPS = Œ£(œÄ(a|x)/Œº * r) / Œ£(œÄ(a|x)/Œº)
    """
    actions_arr = actions.values
    rewards_arr = rewards.values
    
    # –ò–Ω–¥–µ–∫—Å—ã –¥–µ–π—Å—Ç–≤–∏–π
    action_indices = np.array([ACTION_TO_INDEX[a] for a in actions_arr])
    
    # œÄ(a_logged | x)
    pi_logged = policy_probs[np.arange(len(policy_probs)), action_indices]
    
    # –í–∞–∂–Ω–æ—Å—Ç—å (importance weights)
    weights = pi_logged / mu
    
    # SNIPS
    numerator = np.sum(weights * rewards_arr)
    denominator = np.sum(weights)
    
    return numerator / denominator if denominator > 0 else 0.0


def best_static_policy_value(actions: pd.Series, rewards: pd.Series, mu: float = 1/3) -> float:
    """
    –ó–Ω–∞—á–µ–Ω–∏–µ –ª—É—á—à–µ–π —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (–≤—ã–±–∏—Ä–∞–µ—Ç –æ–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ –≤—Å–µ–≥–¥–∞).
    –≠—Ç–æ baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
    
    Best Static IPS = max_a [ E[r | a] ] = max_a [ mean(rewards where action=a) ]
    """
    best_value = -np.inf
    
    for action in ACTIONS:
        mask = actions == action
        if mask.sum() == 0:
            continue
        
        # –°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
        # IPS –æ—Ü–µ–Ω–∫–∞: (sum(r)/mu) / (count/mu) = sum(r) / count = mean(r)
        value = rewards[mask].mean()
        best_value = max(best_value, value)
    
    return best_value


def create_submission(predictions):
    """
    –ü—Ä–æ–ø–∏—à–∏—Ç–µ –∑–¥–µ—Å—å —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ submission.csv –≤ –ø–∞–ø–∫—É results
    !!! –í–ù–ò–ú–ê–ù–ò–ï !!! –§–ê–ô–õ –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –∏–º–µ–Ω–Ω–æ —Ç–∞–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
    """
    # predictions - —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂ (policy_probs, ids)
    policy_probs, ids = predictions
    
    # –°–æ–∑–¥–∞—Ç—å –ø–∞–Ω–¥–∞—Å —Ç–∞–±–ª–∏—Ü—É submission
    submission = pd.DataFrame({
        ID_COL: ids,
        "p_mens_email": policy_probs[:, ACTION_TO_INDEX["Mens E-Mail"]],
        "p_womens_email": policy_probs[:, ACTION_TO_INDEX["Womens E-Mail"]],
        "p_no_email": policy_probs[:, ACTION_TO_INDEX["No E-Mail"]],
    })
    
    os.makedirs('results', exist_ok=True)
    submission_path = 'results/submission.csv'
    submission.to_csv(submission_path, index=False)
    
    print(f"Submission —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {submission_path}")
    logging.info(f"‚úÖ Submission —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {submission_path}")
    
    return submission_path


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã
    
    –í—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω—è—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–¥ —Å–≤–æ–∏ –Ω—É–∂–¥—ã,
    –Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–∑–æ–≤–∏—Ç–µ create_submission() –≤ –∫–æ–Ω—Ü–µ!
    """
    print("=" * 50)
    print("–ó–∞–ø—É—Å–∫ —Ä–µ—à–µ–Ω–∏—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è")
    print("=" * 50)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞–Ω—ã –∑–¥–µ—Å—å)
    config = {
        # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
        "data": {
            "train_path": "data/train.csv",
            "test_path": "data/test.csv",
            "submission_path": "results/submission.csv",
        },
        # Random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        "seed": 42,
        # Logging policy propensity (uniform random = 1/3)
        "mu": 0.3333333333,
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        "model": {
            # –¢–∏–ø –±–∞–∑–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞: 'logistic', 'random_forest', 'extra_trees'
            "type": "logistic",
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å feature engineering (False = —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
            "use_feature_engineering": False,
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            "logistic": {
                "max_iter": 2000,
                "C": 1.0,
                "solver": "lbfgs",
            },
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Random Forest
            "random_forest": {
                "n_estimators": 300,
                "max_depth": None,
                "min_samples_leaf": 5,
                "min_samples_split": 10,
            },
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Extra Trees
            "extra_trees": {
                "n_estimators": 300,
                "max_depth": None,
                "min_samples_leaf": 5,
                "min_samples_split": 10,
            },
        },
        # –ü–æ–ª–∏—Ç–∏–∫–∞ (policy)
        "policy": {
            # –¢–∏–ø –ø–æ–ª–∏—Ç–∏–∫–∏: "greedy" –∏–ª–∏ "softmax"
            "type": "greedy",
            # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è softmax (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ type="softmax")
            # T < 1: –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω–æ, T = 1: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π softmax, T > 1: –±–æ–ª—å—à–µ exploration
            "temperature": 0.1,
            # Epsilon –¥–ª—è epsilon-greedy (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ type="greedy")
            # –ñ–∞–¥–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞: œÄ(a*) = 1 - Œµ, œÄ(other) = Œµ / (n_actions - 1)
            "epsilon": 0.05,
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—è (–¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ SNIPS)
            "min_prob": 0.01,
            # Delta –¥–ª—è override –≤ RL-–æ–±—ë—Ä—Ç–∫–µ
            "override_delta": 0.15,
        },
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        "logging": {
            "level": "INFO",  # DEBUG, INFO, WARNING, ERROR
            "save_experiment_logs": True,
            "experiment_dir": "experiments",
        },
    }
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    set_seed(config["seed"])
    setup_logging(config["logging"]["level"])
    
    logging.info("=" * 60)
    logging.info("CONTEXTUAL BANDIT - SIMPLIFIED BASELINE")
    logging.info("=" * 60)
    logging.info(f"–ú–æ–¥–µ–ª—å: {config['model']['type']}")
    logging.info(f"–ü–æ–ª–∏—Ç–∏–∫–∞: {config['policy']['type']}")
    if config['policy']['type'] == 'greedy':
        logging.info(f"Epsilon: {config['policy']['epsilon']}")
    elif config['policy']['type'] == 'softmax':
        logging.info(f"Temperature: {config['policy']['temperature']}")
    logging.info(f"Seed: {config['seed']}")
    logging.info("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_df, test_df = load_data(config)
    
    # RL-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä—É–∫–∞–º (–¥–µ–π—Å—Ç–≤–∏—è–º) –Ω–∞ train
    reward_means, counts, best_action = compute_action_stats(train_df)
    best_action_idx = ACTION_TO_INDEX[best_action]
    trained_action_mask = np.array([counts[a] > 0 for a in ACTIONS], dtype=bool)
    logging.info("\nüß† –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º (train):")
    for a in ACTIONS:
        logging.info(f"  {a}: count={counts[a]}, mean_reward={reward_means[a]:.5f}")
    logging.info(f"  ‚Üí –õ—É—á—à–∞—è —Ä—É–∫–∞ (baseline): {best_action}")
    
    # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    preprocessor = build_preprocessor()
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (Direct Method)
    logging.info("\nüîß –û–±—É—á–µ–Ω–∏–µ reward –º–æ–¥–µ–ª–µ–π...")
    models = fit_reward_models(train_df, preprocessor, config)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ train –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    logging.info("\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö...")
    train_q_values = predict_q_values(train_df, models)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏
    train_policy = make_policy(train_q_values, config)
    
    # –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏
    snips_value = snips_score(train_policy, train_df[ACTION_COL], train_df[REWARD_COL], config["mu"])
    best_static = best_static_policy_value(train_df[ACTION_COL], train_df[REWARD_COL], config["mu"])
    score = snips_value - best_static
    
    logging.info(f"\nüìà –ú–ï–¢–†–ò–ö–ò:")
    logging.info(f"  SNIPS: {snips_value:.5f}")
    logging.info(f"  Best Static: {best_static:.5f}")
    logging.info(f"  Score (SNIPS - Best Static): {score:.5f}")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ test
    logging.info("\nüéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission...")
    test_q_values = predict_q_values(test_df, models)
    # RL-–æ–±—ë—Ä—Ç–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–ª–∏—Ç–∏–∫–æ–π –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ
    override_delta = config["policy"]["override_delta"]
    epsilon = config["policy"]["epsilon"]
    test_policy = make_rl_wrapped_policy(
        test_q_values,
        baseline_best_action_idx=best_action_idx,
        trained_action_mask=trained_action_mask,
        epsilon=epsilon,
        override_delta=override_delta,
    )
    
    # –í—ã–≤–µ–¥–µ–º –ø—Ä–µ–¥–∏–∫—Ç: –∞—Ä–≥–º–∞–∫—Å –¥–µ–π—Å—Ç–≤–∏–π –∏ –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
    pred_actions_idx = np.argmax(test_policy, axis=1)
    pred_actions = [ACTIONS[i] for i in pred_actions_idx]
    unique, counts_arr = np.unique(pred_actions, return_counts=True)
    logging.info("\nüñ®Ô∏è –ü—Ä–µ–¥–∏–∫—Ç (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ test):")
    for a, c in zip(unique, counts_arr):
        logging.info(f"  {a}: {int(c)}")
    logging.info("–ü—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–≤—ã—Ö 5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (id, action, probs):")
    for i in range(min(5, len(test_df))):
        probs_i = test_policy[i]
        logging.info(
            f"  id={test_df.iloc[i][ID_COL]} ‚Üí {pred_actions[i]} | "
            f"[mens={probs_i[ACTION_TO_INDEX['Mens E-Mail']]:.3f}, "
            f"womens={probs_i[ACTION_TO_INDEX['Womens E-Mail']]:.3f}, "
            f"no={probs_i[ACTION_TO_INDEX['No E-Mail']]:.3f}]"
        )
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏
    logging.info(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ (test):")
    for i, action in enumerate(ACTIONS):
        mean_prob = test_policy[:, i].mean()
        logging.info(f"  {action}: {mean_prob:.3f} (—Å—Ä–µ–¥–Ω–µ–µ)")
    
    logging.info("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞ (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!)
    predictions = (test_policy, test_df[ID_COL])
    create_submission(predictions)
    
    print("=" * 50)
    print("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    print("=" * 50)


if __name__ == "__main__":
    main()